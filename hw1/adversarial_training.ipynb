{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Whsg1XX_OZs6"
   },
   "source": [
    "# Boilerplate\n",
    "\n",
    "Package installation, loading, and dataloaders. There's also a simple model defined. You can change it your favourite architecture if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "R1domTvnONqD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Normalize()\n",
       "  (1): Net(\n",
       "    (fc): Linear(in_features=784, out_features=200, bias=True)\n",
       "    (fc2): Linear(in_features=200, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install tensorboardX\n",
    "\n",
    "import torch\n",
    "import torchattacks\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = False\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 64\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "## Dataloaders\n",
    "train_dataset = datasets.MNIST('mnist_data/', train=True, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    "))\n",
    "test_dataset = datasets.MNIST('mnist_data/', train=False, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    "))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "## Simple NN. You can change this if you want. If you change it, mention the architectural details in your report.\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(28*28, 200)\n",
    "        self.fc2 = nn.Linear(200,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view((-1, 28*28))\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return (x - 0.1307)/0.3081\n",
    "\n",
    "# Add the data normalization as a first \"layer\" to the network\n",
    "# this allows us to search for adverserial examples to the real image, rather than\n",
    "# to the normalized image\n",
    "model = nn.Sequential(Normalize(), Net())\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCmWfZHTO8Oo"
   },
   "source": [
    "# Implement the Attacks\n",
    "\n",
    "Functions are given a simple useful signature that you can start with. Feel free to extend the signature as you see fit.\n",
    "\n",
    "You may find it useful to create a 'batched' version of PGD that you can use to create the adversarial attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EZjvA49yONqP"
   },
   "outputs": [],
   "source": [
    "# The last argument 'targeted' can be used to toggle between a targeted and untargeted attack.\n",
    "def fgsm(model, x, y, eps_step, eps):\n",
    "    # Notes: put the model in eval() mode for this function\n",
    "    model.eval()\n",
    "    x_copy = x.clone().detach().to(device)\n",
    "    x_copy.requires_grad = True\n",
    "    #perform gradient descent\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "    outputs = model(x_copy)\n",
    "    loss = loss_fun(outputs, y)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    gradient = x_copy.grad.data\n",
    "    perturbation = eps_step * gradient.sign()\n",
    "    x_copy = x_copy + perturbation\n",
    "    #project back to epsilon ball\n",
    "    delta = x_copy - x\n",
    "    delta = torch.clamp(delta, -eps, eps)\n",
    "    x_copy = x + delta\n",
    "    x_copy = torch.clamp(x_copy, 0, 1).detach()\n",
    "    return x_copy\n",
    "    \n",
    "def pgd_untargeted(model, x, y, k, eps, eps_step):\n",
    "    # Notes: put the model in eval() mode for this function\n",
    "    model.eval()\n",
    "    for i in range(k):\n",
    "        x = fgsm(model, x, y, eps_step, eps)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Mja_AB4RykO"
   },
   "source": [
    "# Implement Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs, enable_defense=True, attack='pgd', eps=0.1, eps_step=0.01, k=15):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_clean = 0\n",
    "        correct_adv = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output_clean = model(data)\n",
    "            _, pred_clean = output_clean.max(1)\n",
    "            correct_clean += (pred_clean == target).sum().item()\n",
    "\n",
    "            \n",
    "            if enable_defense and attack == 'pgd':\n",
    "                data.requires_grad = True\n",
    "                data_adv = pgd_untargeted(model, data, target, k, eps, eps_step)\n",
    "                output_adv = model(data_adv)\n",
    "                _, pred_adv = output_adv.max(1)\n",
    "                correct_adv += (pred_adv == target).sum().item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data_adv if enable_defense and attack == 'pgd' else data)\n",
    "            loss = loss_fun(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total += target.size(0)\n",
    "\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        standard_accuracy = 100. * correct_clean / total\n",
    "        robust_accuracy = 100. * correct_adv / total if enable_defense else None\n",
    "\n",
    "        if enable_defense:\n",
    "            print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}, '\n",
    "                  f'Standard Accuracy: {standard_accuracy:.2f}%, Robust Accuracy: {robust_accuracy:.2f}%')\n",
    "        else:\n",
    "            print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}, '\n",
    "                  f'Standard Accuracy: {standard_accuracy:.2f}%')\n",
    "\n",
    "    print('Training complete!')\n",
    "\n",
    "\n",
    "def test_model(model, eps=0.1, eps_step=0.01, k=15, attack='pgd'):\n",
    "    correct_adv = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    \n",
    "    if attack == 'cw':\n",
    "        cw = torchattacks.CW(model, c=1, kappa=0, steps=300, lr=eps_step)\n",
    "    \n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        if attack == 'pgd':\n",
    "            data_adv = pgd_untargeted(model, data, target, k, eps, eps_step)\n",
    "        elif attack == 'cw':\n",
    "            data_adv = cw(data, target)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_adv = model(data_adv)\n",
    "            _, pred_adv = output_adv.max(1)\n",
    "            correct_adv += (pred_adv == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    \n",
    "    adversarial_accuracy = 100. * correct_adv / total\n",
    "    print(f'Adversarial Accuracy: {adversarial_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_on_attacks(model,attack='pgd', eps=.3):\n",
    "    # use pgd_untargeted() within this function\n",
    "    if attack == 'pgd':\n",
    "        test_model(model,eps,.01, k=15)\n",
    "    elif attack == 'cw':\n",
    "        test_model(model,attack='cw', eps_step=eps)\n",
    "    elif attack == 'fgsm':\n",
    "        test_model(model, eps, .01, k=1, attack='pgd')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPMdfEhtR3zm"
   },
   "source": [
    "# Study Accuracy, Quality, etc.\n",
    "\n",
    "Compare the various results and report your observations on the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.0743\n",
      "Epoch 1, Loss: 0.2393, Standard Accuracy: 92.95%\n",
      "Epoch [2/5], Loss: 0.0266\n",
      "Epoch 2, Loss: 0.1007, Standard Accuracy: 96.87%\n",
      "Epoch [3/5], Loss: 0.0116\n",
      "Epoch 3, Loss: 0.0684, Standard Accuracy: 97.86%\n",
      "Epoch [4/5], Loss: 0.0348\n",
      "Epoch 4, Loss: 0.0509, Standard Accuracy: 98.39%\n",
      "Epoch [5/5], Loss: 0.1371\n",
      "Epoch 5, Loss: 0.0398, Standard Accuracy: 98.71%\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "## train the original model\n",
    "model = nn.Sequential(Normalize(), Net())\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_model(model, 5, False)\n",
    "torch.save(model.state_dict(), 'weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Accuracy: 53.34%\n",
      "Adversarial Accuracy: 4.14%\n",
      "Adversarial Accuracy: 4.14%\n",
      "Adversarial Accuracy: 4.14%\n"
     ]
    }
   ],
   "source": [
    "## PGD attack\n",
    "model = nn.Sequential(Normalize(), Net())\n",
    "model.load_state_dict(torch.load('weights.pt'))\n",
    "\n",
    "for eps in [0.005, 0.01, 0.05, 0.1]:\n",
    "    test_model_on_attacks(model, attack='pgd',eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ufD-ccTFR8R2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.0349\n",
      "Epoch 1, Loss: 1.1486, Standard Accuracy: 89.34%, Robust Accuracy: 60.50%\n",
      "Epoch [2/5], Loss: 0.9489\n",
      "Epoch 2, Loss: 0.8060, Standard Accuracy: 95.45%, Robust Accuracy: 72.88%\n",
      "Epoch [3/5], Loss: 0.9935\n",
      "Epoch 3, Loss: 0.7283, Standard Accuracy: 96.17%, Robust Accuracy: 75.78%\n",
      "Epoch [4/5], Loss: 0.7448\n",
      "Epoch 4, Loss: 0.6861, Standard Accuracy: 96.57%, Robust Accuracy: 77.32%\n",
      "Epoch [5/5], Loss: 0.9215\n",
      "Epoch 5, Loss: 0.6632, Standard Accuracy: 96.72%, Robust Accuracy: 78.10%\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "## PGD based adversarial training\n",
    "model = nn.Sequential(Normalize(), Net())\n",
    "train_model(model, 5, True, 'pgd')\n",
    "torch.save(model.state_dict(), 'weights_AT.pt')\n",
    "mn= 'weights_AT.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGD based adversarial atack\n",
      "Adversarial Accuracy: 92.04%\n",
      "Adversarial Accuracy: 78.43%\n",
      "Adversarial Accuracy: 78.43%\n",
      "Adversarial Accuracy: 78.43%\n",
      "C&W based adversarial atack\n",
      "Adversarial Accuracy: 76.87%\n",
      "FGSM based adversarial atack\n",
      "Adversarial Accuracy: 96.87%\n",
      "Adversarial Accuracy: 96.55%\n",
      "Adversarial Accuracy: 96.55%\n",
      "Adversarial Accuracy: 96.55%\n"
     ]
    }
   ],
   "source": [
    "## PGD attack\n",
    "model = nn.Sequential(Normalize(), Net())\n",
    "model.load_state_dict(torch.load(mn))\n",
    "print('PGD based adversarial atack')\n",
    "for eps in [0.005, 0.01, 0.05, 0.1]:\n",
    "    test_model_on_attacks(model, attack='pgd',eps=eps)\n",
    "\n",
    "#C&W attack\n",
    "print('C&W based adversarial atack')\n",
    "test_model_on_attacks(model, attack='cw', eps=.8)\n",
    "\n",
    "#FGSM\n",
    "print('FGSM based adversarial atack')\n",
    "for eps in [0.005, 0.01, 0.05, 0.1]:\n",
    "    test_model_on_attacks(model, attack='fgsm',eps=eps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 \n",
    "The final standard accuracy for the model trained soley on the training set was 98.71%, but the standard accuracy for the model trained on the robust data set generated from PGD was 96.72%. This is in line with what we learned in class that training for increased robustness leads to a dip in performance on overall accuracy. After adverserial trainng the model achieved 78.10% robust accuracy. Attacking the non-trained model revealed adverserial accuracy of 53.34%, and 4.14% from eps ranges of .005 and .01, respecively. The model that was trained to be robust started with a 92.04% adverserial accuracy and went down to a 78.43% accuracy. This shows that the model to be robust greatly improved its robustness and shows that the adverserial examples were effective enough to make the unrobust-model succeptible to the PGD attack.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "The PGD attack was able to achieve an optimal adverserial accuracy of 78.4% on the robust model and 4.14% on the normally trained model. This shows that the robust training was succesful in making the model more robust against PGD attacks. The FGSM attack was able to achieve an optimal 06.55% adverserial accuracy against the robust model showing how its a less effective way to attack a model. The C&W attack was able to achieve an adverserial accuracy of 76.87% against the robust model. Although, the C&W attack is known to be better at generating adverserial examples, hardware contraints and hyperparameter tuning could have playd into its worse performance compared to the PGD attack. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "#### Paper\n",
    "https://arxiv.org/pdf/1905.02175\n",
    "\n",
    "#### Summary\n",
    "The paper \"Adversarial Examples Are Not Bugs, They Are Features\" discusses their findings that adversarial examples are not flukes or statistical anomalies but are formed from the way machine learning models learn to rely on non-robust features, predictive patterns in data that can’t be understood by humans but highly effective for models. The paper argues these features make models vulnerable to adversarial attacks because they exploit non-robust features. The authors provide a theoretical framework, supporting their claims and showing that adversarial vulnerability is derived from models' tendency to focus on accuracy rather than robustness. The paper shows that models trained with non-robust features perform well in standard settings but poorly in adversarial settings by constructing datasets that differentiate between robust and non-robust features.\n",
    "\n",
    "#### Strengths\n",
    "One strength I particularly appreciated was the paper's new approach in understanding how adversarial examples are created. The paper focused on how adversarial examples occur because of non-robust features rather than from rare statistical occurrences. This is a great discovery that has tangible benefits in making models more robust and less susceptible to adversarial attacks. Another strength in the paper is its theoretical model that is used to explain adversarial examples which then leads to empirically disentangling robust and non-robust features in real-world datasets. This framework can help understand what features help sustain robustness and how to train a model to be more robust in the presence of adversarial examples. Finally, the paper offers concrete datasets and models, showing how their findings perform when being applied. They show how their robust training leads to robust accuracy 20 times greater than under standard training.\n",
    "\n",
    "#### Weaknesses\n",
    "One potential weakness is that the experiments in the paper are mainly focused on well-known datasets like CIFAR-10 and ImageNet, that limit how the conclusions of the paper can be applied to more complicated tasks or real world problems. A potential fix is extending the experiments to a wider variety of datasets, potentially with different modalities, and using different ML models to evaluate how their findings stand up in different problem settings. Another potential weakness is that the paper mentions the trade off between robustness and accuracy but doesn’t go into too much analysis on it. More understanding is needed in terms of how model architecture, loss functions and training strategies play into this phenomenon. To address this the paper could look into the theoretical aspects of this topic and come up with different training approaches that they can then test to see their effects on robustness vs accuracy. Finally, some of the assumptions held in the paper’s theoretical framework may not hold for all data types or machine learning models, limiting the scope of the findings. More careful analysis into the assumptions and how they fit other ML architectures could address this issue.\n",
    "\n",
    "#### Extensions\n",
    "A possible extension of this work could be investigating the role of non-robust features in adversarial attacks on large language models. By exploring how non-robust features in text data contribute to adversarial vulnerabilities in open-source LLMs like Ollama. Understanding the types of non-robust features that LLMs rely on could lead to improved adversarial training techniques for these models. LLMs are very popular models and have the potential to cause a lot of destruction if people with the wrong intentions can manipulate them in nefarious ways so this extension is of great importance."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
